{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jira import JIRA\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from hyperdash import monitor_cell\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pprint\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Links\n",
    "\n",
    "https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "http://jira.readthedocs.io/en/latest/examples.html#comments\n",
    "\n",
    "\n",
    "https://www.codeproject.com/Articles/11835/WordNet-based-semantic-similarity-measurement\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "https://pypi.org/project/sematch/1.0.3/\n",
    "\n",
    "https://stackoverflow.com/questions/16877517/compare-similarity-of-terms-expressions-using-nltk\n",
    "\n",
    "https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python\n",
    "\n",
    "https://stackoverflow.com/questions/44589872/doc2vec-pull-documents-from-inferred-document\n",
    "\n",
    "https://www.quora.com/Is-doc2vec-suitable-in-information-retrieval-to-calculate-distance-between-query-and-doc\n",
    "\n",
    "https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "\n",
    "https://stackoverflow.com/questions/47022246/warning-message-after-importing-gensim-module-in-windows-os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "maxResults = 2000\n",
    "jira_url = 'https://jira.endurance.com'\n",
    "username = 'bhavul.g'\n",
    "password = 'insert-your-password-here'\n",
    "filter_fnb_tickets = 'project = WSE AND status in (Done, Completed) AND (component in (\"Support - FnB\") OR \"Customer Request Type\" = \"Support - FnB\") AND component not in (\"Ad hoc\") ORDER BY updated DESC, created ASC, component ASC, priority DESC'\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer('english')\n",
    "ignored_words_string = \"reseller, customer, error, c, r, issue, client, ticket, https, screenshot, support, following, name, getting, kindly, please, reference, see, prntscr, information, trying, need, ref, one, hence, pm, jira, adhocnfoway, www, wse, endurance, gmail, says, thank, png, jpg, jpeg, gif, paye, jul, e, still, color, replicate, 2fwww, 2fservlet, refer, dnot, gets, two, april, entire, asked, many, want, fine, attachment, told, understand, regarding, take, keep, latest, someone, science, hello, might, rahul, backend, yesterday, answer, searched\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def remove_code_from_comments(comment_body):\n",
    "    return re.sub(r'{code:.*}[\\s\\S]*?{code}','',str(comment_body))\n",
    "\n",
    "def get_jira_issue_object(authed_jira, jira_name):\n",
    "    return authed_jira.issue(jira_name)\n",
    "\n",
    "def get_title(jira_issue_object):\n",
    "    return jira_issue_object.fields.summary\n",
    "\n",
    "def get_summary(jira_issue_object):\n",
    "    return jira_issue_object.fields.description\n",
    "\n",
    "def get_jira_id(jira_issue_object):\n",
    "    return jira_issue_object.key\n",
    "\n",
    "def get_status(jira_issue_object):\n",
    "    return jira_issue_object.fields.status\n",
    "\n",
    "def get_list_of_comments(jira_issue_object):\n",
    "    return jira_issue_object.fields.comment.comments\n",
    "\n",
    "def get_reqd_comments_data(list_of_comments):\n",
    "    ticket_dict = {}\n",
    "    ticket_dict['comments_data'] = [] \n",
    "    ticket_dict['comments_corpus'] = []\n",
    "    for comment in list_of_comments:\n",
    "        comment_data = {}\n",
    "        comment_data['emailAddress'] = comment.author.emailAddress\n",
    "        comment_data['body'] = comment.body\n",
    "        comment_data['created'] = comment.created\n",
    "        comment_data['updated'] = comment.updated\n",
    "        ticket_dict['comments_data'].append(comment_data)\n",
    "        comment_corpus_data = remove_code_from_comments(comment_data['body'])\n",
    "        ticket_dict['comments_corpus'].append(comment_corpus_data)\n",
    "    return ticket_dict['comments_data'],ticket_dict['comments_corpus']\n",
    "\n",
    "def filter_crawler(authed_jira, jira_filter):\n",
    "    print(\"Crawling the filter...\")\n",
    "    filter_tickets = authed_jira.search_issues(jira_filter, maxResults=maxResults)\n",
    "    tickets_corpus = []\n",
    "    for ticket in filter_tickets:\n",
    "        ticket_dict = {}\n",
    "        jira_id = get_jira_id(ticket)\n",
    "        ticket_full_data = authed_jira.issue(jira_id)  \n",
    "        ticket_dict['jiraid'] = jira_id\n",
    "        ticket_dict['title'] = get_title(ticket_full_data) \n",
    "        ticket_dict['summary'] = get_summary(ticket_full_data)\n",
    "        list_of_comments = get_list_of_comments(ticket_full_data)\n",
    "        ticket_dict['comments_data'],ticket_dict['comments_corpus'] = get_reqd_comments_data(list_of_comments)\n",
    "        tickets_corpus.append(ticket_dict)\n",
    "    print(\"Crawling done.\")\n",
    "    return tickets_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features yet to Add\n",
    "\n",
    "- Automatic model improvement using human input (cron that checks for corrected bots comments)\n",
    "- Take assignee's name into picture while calculating the similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAN\n",
    "\n",
    "Actual Thing that runs every 1 hour: \n",
    "- find all open issues and get their names\n",
    "- Maintain a list for which you've alrdy done, and now make api call only for one you've not done\n",
    "- for the new ones, Get their body\n",
    "- Find similar ones via gensim model\n",
    "\n",
    "\n",
    "2nd cron that runs once a week:\n",
    "- Find all done issues > some creation date and get their names\n",
    "- Check in the list you maintain the issues which are correctly marked done\n",
    "- Figure out any new ones\n",
    "- Mail those people, as well as dhanya.n and swapnil.b and joel.r\n",
    "\n",
    "3rd cron that runs once a week:\n",
    "- Make a corpus of data of all documents till last week\n",
    "- Ignore bots comments in that\n",
    "- train new model on this\n",
    "- SOMEHOW CHECK IF THIS IS BETTER THAN PREVIOUS OR NOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "authed_jira = JIRA(jira_url,auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling the filter...\n",
      "Crawling done.\n",
      "This run of \"crawling jirabot\" ran for 0:03:05 and logs are available locally at: /Users/bhavul.g/.hyperdash/logs/crawling-jirabot/crawling-jirabot_2018-04-22t07-51-22-709265.log\n"
     ]
    }
   ],
   "source": [
    "%%monitor_cell \"crawling jirabot\"\n",
    "fnb_tickets_corpus = filter_crawler(authed_jira=authed_jira, jira_filter=filter_fnb_tickets)\n",
    "tagged_data_only_summary = [TaggedDocument(words=word_tokenize((str(ticket_dict['title'])+\" \"+str(ticket_dict['summary'])).lower()), tags=[str(ticket_dict['jiraid'])]) for ticket_dict in fnb_tickets_corpus]\n",
    "tagged_data_all_text = [TaggedDocument(words=word_tokenize((str(ticket_dict['title'])+\" \"+str(ticket_dict['summary'])+\" \"+str(ticket_dict['comments_corpus'])).lower()), tags=[str(ticket_dict['jiraid'])]) for ticket_dict in fnb_tickets_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(model_name_prefix,tagged_data, vec_size, alpha, min_word_count_per_doc, dm, no_of_epochs):\n",
    "    model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=alpha,\n",
    "                min_count=min_word_count_per_doc,\n",
    "                dm =dm,\n",
    "                workers = 4)\n",
    "    \n",
    "    model.build_vocab(tagged_data)\n",
    "    \n",
    "    for epoch in range(no_of_epochs):\n",
    "        model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    modelname = model_name_prefix+\"_d2v_\"+str(dm)+\"dm_\"+str(no_of_epochs)+\"epoch_\"+str(vec_size)+\"vecsize_\"+str(alpha)+\"alpha.model\"\n",
    "    model.save(modelname)\n",
    "    print(\"Model trained:\",modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%monitor_cell \"doc2vec Models jirabot\"\n",
    "warnings.filterwarnings(action='once')\n",
    "epochs_list = [100,500,1000]\n",
    "size_list = [10,50,100,200]\n",
    "alpha_list = [0.05, 0.01, 0.025]\n",
    "dm_list = [0,1]\n",
    "\n",
    "i = 1\n",
    "for vec_size in size_list:\n",
    "    for alpha in alpha_list:\n",
    "        for epoch in epochs_list:\n",
    "            for dm in dm_list:\n",
    "                train_doc2vec_model(str(i)+\"_summary\",tagged_data_only_summary, vec_size, alpha, 1, dm, epoch)\n",
    "                train_doc2vec_model(str(i)+\"_alldata\",tagged_data_all_text, vec_size, alpha, 1, dm, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_list = ['1_alldata_d2v_0dm_1000epoch_100vecsize_0.025alpha.model']\n",
    "# tickets_dev_set = ['WSE-2607']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>For</th>\n",
       "      <th>Modelname</th>\n",
       "      <th>Points</th>\n",
       "      <th>Related-Tickets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST-123</td>\n",
       "      <td>TestModel</td>\n",
       "      <td>-10</td>\n",
       "      <td>TEST;TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        For  Modelname  Points Related-Tickets\n",
       "0  TEST-123  TestModel     -10       TEST;TEST"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Modelname':['TestModel'], 'For':['TEST-123'], 'Related-Tickets':['TEST;TEST'], 'Points':[-10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%monitor_cell \"doc2vec Models jirabot - devset\"\n",
    "\n",
    "tickets_dev_set = ['WSE-2607','WSE-3608','OFB-457','OFB-557','OFB-582','OFB-634','OFB-1002','OFB-1005','OFB-1030']\n",
    "models_list = os.listdir('./models')\n",
    "for modelname in models_list:\n",
    "    print(\"processing \",modelname,\"....\")\n",
    "    model = Doc2Vec.load(\"./models/\"+str(modelname))\n",
    "    for ticket in tickets_dev_set:\n",
    "        data_dict = {}\n",
    "        test_issue_text_data = ''\n",
    "        data_dict['Modelname'] = modelname\n",
    "        data_dict['Points'] = 0\n",
    "\n",
    "        test_issue = authed_jira.issue(ticket)\n",
    "        data_dict['For'] = get_jira_id(test_issue)\n",
    "        \n",
    "        test_issue_text_data += str(get_title(test_issue))\n",
    "        test_issue_text_data += str(get_summary(test_issue))\n",
    "        list_of_comments_test = get_list_of_comments(test_issue)\n",
    "        for comment in list_of_comments_test:\n",
    "            test_issue_text_data += str(remove_code_from_comments(comment.body))\n",
    "        \n",
    "        test_data = word_tokenize(test_issue_text_data.lower())\n",
    "        test_issue_vector = model.infer_vector(test_data)\n",
    "        similar_doc = model.docvecs.most_similar(positive=[test_issue_vector], topn=5)\n",
    "        data_dict['Related-Tickets'] = \":\".join([\"(\"+str(doc)+\"|\"+str(\"{0:.3f}\".format(similar_score))+\")\" for (doc,similar_score) in similar_doc])\n",
    "        df.loc[df.index.max() + 1] = data_dict\n",
    "    print(\"all done for model - \",modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dev_set_outputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelname</th>\n",
       "      <th>For</th>\n",
       "      <th>Related-Tickets</th>\n",
       "      <th>Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Modelname For Related-Tickets Points\n",
       "NaN         A   B               C     25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful links\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim/issues/1560\n",
    "    \n",
    "https://marketplace.atlassian.com/plugins/com.deniz.jira.similarissues/server/overview\n",
    "    \n",
    "http://www.diss.fu-berlin.de/docs/servlets/MCRFileNodeServlet/FUDOCS_derivate_000000003281/JiraExpertFinder.pdf\n",
    "    \n",
    "https://www.kaggle.com/currie32/predicting-similarity-tfidfvectorizer-doc2vec\n",
    "    \n",
    "https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity/12128777\n",
    "    \n",
    "https://docs.google.com/spreadsheets/d/14DFm-6wamaaOMN0b-4tYd5gOtHpqzdytLT-KUwLuolU/edit#gid=1607381971\n",
    "\n",
    "https://stackoverflow.com/questions/49134593/how-to-expand-the-words-of-tfidf-vectorizer-in-sklearn-without-retraining-the-wh\n",
    "\n",
    "https://www.quora.com/How-does-doc2vec-represent-feature-vector-of-a-document-Can-anyone-explain-mathematically-how-the-process-is-done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE. TO KNOW IF ALL_WORDS that are getting out are correct ones.\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pprint\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "ignored_words_list = [x.strip() for x in ignored_words_string.split(',')]\n",
    "words_data = pd.DataFrame({'word':['test'],'count':[-1]})\n",
    "final_corpus = []\n",
    "all_words = []\n",
    "list_of_docs = []\n",
    "for ticket_dict in fnb_tickets_corpus:\n",
    "    doc_cleaned_text = ''\n",
    "    words = (str(ticket_dict['title'])+\" \"+str(ticket_dict['summary'])).lower()\n",
    "    words = re.sub('\\\\b\\d+(?:\\.\\d+)?\\s*', '', words)      # remove a number or decimal num followed by a space\n",
    "    words = re.sub('[rc]*id\\s*:*', '', words)              # remove rid, id fields\n",
    "    words = re.split('\\W+', words)\n",
    "    words = ' '.join(words)\n",
    "    words = words.split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [w for w in words if not w in ignored_words_list]\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    all_words += stemmed_words\n",
    "    doc_cleaned_text = ' '.join(stemmed_words)\n",
    "    list_of_docs.append(doc_cleaned_text)\n",
    "    final_corpus.append({'jiraid':ticket_dict['jiraid'], 'words':doc_cleaned_text})\n",
    "\n",
    "words_dict = Counter(all_words)\n",
    "words_dict = dict(words_dict)\n",
    "for key in words_dict.keys():\n",
    "    if not words_dict[key] == 1:\n",
    "        words_data.loc[words_data.index.max()+1] = [words_dict[key],key]\n",
    "words_data.to_csv('words_frequencies1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  50, 156, 165])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "tfidf = tfidfvectorizer.fit_transform(list_of_docs)\n",
    "tfidf_test = tfidfvectorizer.transform()\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "related_docs_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions - TfIdf Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(document_of_words):\n",
    "    document_of_words = document_of_words.lower()\n",
    "    document_of_words = re.sub('\\\\b\\d+(?:\\.\\d+)?\\s*', '', document_of_words)      # remove a number or decimal num followed by a space\n",
    "    document_of_words = re.sub('[rc]*id\\s*:*', '', document_of_words)              # remove rid, id fields\n",
    "    document_of_words = re.split('\\W+', document_of_words)                        # remove all non-words (make a list)\n",
    "    document_of_words = [w for w in document_of_words if not w in stops]           # remove stop words\n",
    "    document_of_words = [w for w in document_of_words if not w in ignored_words_list]    # remove ignored words\n",
    "    stemmed_words = [stemmer.stem(word) for word in document_of_words]       # stem each word\n",
    "    return ' '.join(stemmed_words)\n",
    "    \n",
    "\n",
    "def extract_clean_documents_from_corpus(corpus):\n",
    "    print(\"Extracting and Cleaning documents...\")\n",
    "    final_corpus = []\n",
    "    list_of_docs = []\n",
    "    i = 0\n",
    "    for ticket_dict in corpus:\n",
    "        print(\"Processing \",ticket_dict['title'])\n",
    "        doc_cleaned_text = ''\n",
    "        document_of_words = (str(ticket_dict['title'])+\" \"+str(ticket_dict['summary']))\n",
    "        doc_cleaned_text = clean_document(document_of_words)\n",
    "        list_of_docs.append(doc_cleaned_text)\n",
    "        final_corpus.append({'jiraid':ticket_dict['jiraid'], 'words':doc_cleaned_text, 'index':i})\n",
    "        i+=1\n",
    "    return list_of_docs,final_corpus\n",
    "\n",
    "def find_top_n_similar_documents(n,tfidf_test,tfidf_trainingset,cleaned_training_corpus):\n",
    "    cosine_similarities = linear_kernel(tfidf_test, tfidf_trainingset).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-n:-1]\n",
    "    related_jira_ids = []\n",
    "    for ticket in cleaned_training_corpus:\n",
    "        if(ticket['index'] in related_docs_indices):\n",
    "            related_jira_ids.append(ticket['jiraid'])\n",
    "    return related_docs_indices,related_jira_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting To JIRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "authed_jira = JIRA(jira_url,auth=(username, password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling the filter...\n",
      "Crawling done.\n",
      "This run of \"crawling jirabot\" ran for 0:05:17 and logs are available locally at: /Users/bhavul.g/.hyperdash/logs/crawling-jirabot/crawling-jirabot_2018-04-22t12-27-04-970127.log\n"
     ]
    }
   ],
   "source": [
    "%%monitor_cell \"crawling jirabot\"\n",
    "fnb_tickets_corpus = filter_crawler(authed_jira=authed_jira, jira_filter=filter_fnb_tickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer()\n",
    "list_of_docs,training_ticket_corpus = extract_clean_documents_from_corpus(fnb_tickets_corpus)\n",
    "tfidf_trainingset = tfidf_model.fit_transform(list_of_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " WSE-3150  >>>>  ['WSE-3150', 'WSE-3245', 'WSE-2324', 'WSE-2368'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tickets_dev_set = ['WSE-2607','WSE-3608','OFB-457','OFB-557','OFB-582','OFB-634','OFB-1002','OFB-1005','OFB-1030']\n",
    "tickets_dev_set = ['WSE-3150']\n",
    "for ticket in tickets_dev_set:\n",
    "    test_issue = authed_jira.issue(ticket)\n",
    "    title = get_title(test_issue)\n",
    "    summary = get_summary(test_issue)\n",
    "    document_test = str(title)+\" \"+str(summary)\n",
    "    cleaned_document = clean_document(document_test)\n",
    "    cleaned_document = [cleaned_document]\n",
    "    tfidf_test = tfidf_model.transform(cleaned_document)\n",
    "    related_indices, related_jiras = find_top_n_similar_documents(5,tfidf_test[0:1],tfidf_trainingset,training_ticket_corpus)\n",
    "    print(\"\\n\",ticket,\" >>>> \",related_jiras,\"\\n\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
